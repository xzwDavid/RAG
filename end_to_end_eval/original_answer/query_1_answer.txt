:**

To build a comprehensive real-time data processing application using Spark, you need to integrate several components and concepts from the Spark ecosystem:

1. **Data Ingestion and Streaming Processing:**
   - Use **Structured Streaming** for real-time data processing. This newer API replaces the legacy Spark Streaming and offers a more integrated approach with DataFrames and Datasets. It allows you to process data streams in real-time using SQL-like queries, which is crucial for ingesting and processing data in real-time.
   - For the structured streaming data source, you can use Kafka or Kinesis, which are commonly used for real-time data feeds. You can set up Spark to consume these streams using Structured Streaming.

2. **Data Processing and Analysis:**
   - Utilize **Spark SQL and DataFrames** for processing structured data. DataFrames provide an optimized API over RDDs and can be used to perform complex transformations and aggregations efficiently. They are particularly useful for working with tables and relational data.
   - Implement **MLlib** for machine learning tasks. MLlib offers various algorithms that can be applied to the streaming data to perform real-time analytics. For example, you could use MLlib to implement predictive models or anomaly detection on the streaming data.

3. **Graph Processing:**
   - If your application requires graph processing, use **GraphX**. Although GraphX is older than Structured Streaming, it offers powerful graph analytics capabilities that can be applied to the data once it's been processed into a graph format.

4. **Deployment on a Cluster:**
   - Deploy your Spark application using the **Kubernetes deployment mode**. Kubernetes is ideal for managing Spark applications in a containerized environment, which is essential for scaling and managing the resources efficiently.
   - You can use the **Spark Kubernetes Operator** to simplify the deployment and management of Spark applications in Kubernetes. This operator provides resources like SparkApp and SparkCluster to facilitate the deployment and monitoring of Spark jobs.

5. **Monitoring and Optimization:**
   - Use Spark's **Web UI and Monitoring Tools** to track the performance and resource usage of your application. This is crucial for identifying bottlenecks and optimizing the processing power.
   - Implement **Tuning and Configuration** strategies to optimize the performance of your Spark jobs. This includes adjusting parameters like executor memory, parallelism, and tuning the SQL queries for better execution plans.

By leveraging these components and deployment strategies, you can create a robust real-time data processing application that can scale efficiently and provide insights in real-time using Spark's various APIs and deployment modes.