To optimize the processing and performance of a Spark application that counts the maximum number of words in a line and computes word frequencies, while deploying it on a cluster, you need to leverage Spark's caching mechanisms, configuration settings, and deployment capabilities. Here's how you can achieve this in a comprehensive manner:

1. **Use Caching for Efficient Data Access:**
   - Implement caching for the dataset you are processing. For instance, if you are using Datasets in Spark SQL, you can cache the dataset using the `cache()` method. This will pull the data into a cluster-wide in-memory cache, which is beneficial if the dataset is accessed multiple times.
   - Example in Scala:
     ```scala
     val wordCounts = textFile.flatMap(line => line.split(" ")).groupByKey(identity).count()
     wordCounts.cache()
     ```

2. **Configure Spark for Optimal Performance:**
   - Adjust Spark configurations to optimize resource usage. For example, you can set the value of `spark.sql.shuffle.partitions` to an appropriate number to control the parallelism during shuffles, which is crucial for operations like `groupByKey`.
   - Example of configuration via `SparkConf`:
     ```scala
     val conf = new SparkConf().setAppName("WordCountApp").setMaster("local[2]")
     conf.set("spark.sql.shuffle.partitions", "50")
     ```

3. **Deploy the Application on a Cluster:**
   - Package your application and submit it to a Spark cluster using `spark-submit`. This can be a standalone cluster or one managed by YARN, Kubernetes, or another cluster manager.
   - Example command to submit the application:
     ```bash
     ./bin/spark-submit --class org.apache.spark.examples.SparkPi \
       --master "local[4]" \
       --conf spark.eventLog.enabled=false \
       --conf spark.executor.extraJavaOptions=-XX:+PrintGCDetails \
       myApp.jar
     ```

4. **Monitor and Optimize Further:**
   - Use Spark's monitoring tools to track the performance and resource usage of your application. The Web UI provides insights into how each task is performing, which can help in identifying bottlenecks.
   - Consider tuning additional configurations, such as `spark.sql.files.openCostInBytes` to manage the cost of opening files efficiently.

By combining these strategies, you can significantly enhance the performance and efficiency of your Spark application, ensuring that it runs smoothly on a cluster environment.