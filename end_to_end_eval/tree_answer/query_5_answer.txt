To configure and run a Spark application in Python on a cluster with efficient shared variable usage, memory management, and fault tolerance, follow these steps:

1. **Set Up Spark Properties and Environment:**
   - Use the `SparkConf` object to set up Spark properties. You can dynamically load configurations using the `spark-submit` command with the `--conf` flag. For instance:
     ```bash
     ./bin/spark-submit \
       --master "cluster_master_url" \
       --deploy-mode cluster \
       --conf spark.eventLog.enabled=false \
       --conf spark.executor.memory=4g \
       --conf spark.executor.extraJavaOptions=-XX:+PrintGCDetails \
       myApp.py
     ```
   - Ensure that the `spark.executor.memory` is set to accommodate your dataset and application needs.

2. **Use Shared Variables:**
   - For shared variables, use broadcast variables when you need to cache a value in memory on all nodes and accumulators for counters or sums. You can define these in your Python application:
     ```python
     from pyspark import SparkContext, SparkConf, Broadcast, Accumulator

     sc = SparkContext(conf=SparkConf().setMaster("cluster_master_url").setAppName("MyApp"))
     broadcastVar = Broadcast(someLargeDataset)
     accum = Accumulator(0)
     ```

3. **Optimize Memory Usage:**
   - Spark automatically handles garbage collection and memory management, but you can help by ensuring that your RDDs are cached when needed and persisted in memory:
     ```python
     linesWithSpark.cache()
     linesWithSpark.persist()
     ```
   - Use the `spark.memory.fraction` and `spark.memory.storageFraction` properties to control how much memory is allocated to executors and storage, relative to the total executor memory.

4. **Fault Tolerance and Executor Decommissioning:**
   - Spark's RDDs and Datasets are fault-tolerant, meaning they can automatically recover from node failures. However, ensure that your `SparkContext` is not stopped until all tasks are completed, using `spark.stop()` only when all data is collected:
     ```python
     sc.stop()
     ```
   - When decommissioning executors, Spark will automatically rebalance data across the remaining executors, as long as the `spark.executor.maxNumCores` setting is not exceeded on any node.

5. **Submit the Application:**
   - Package your Python application and submit it using the `spark-submit` script, ensuring that all configurations and dependencies are included.

By following these steps, you ensure that your Spark application in Python is efficiently configured and run on a cluster, with proper handling of shared variables, memory management, and fault tolerance. This setup allows the application to scale and adapt to resource changes dynamically.