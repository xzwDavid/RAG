:**

To create, process, and deploy a real-time data processing application using Spark that combines streaming data, machine learning, and graph analytics, you would need to integrate several components and concepts from the Spark ecosystem. Here's a detailed explanation:

1. **Creating the Streaming Data Source:**
   - **Streaming Data Source:** Use Spark Streaming to create a data source that ingests real-time data streams. This can be done using various input sources like Kafka, Kinesis, or TCP sockets. For instance, you can set up Spark Streaming to receive data streams from a TCP socket, which might be output from a sensor or a web application.
   - **Example Code:** You would need to initialize a `StreamingContext` with a batch interval that determines how often data is processed. For example, using Python, you might set up a socket text stream and process it in batches of 1 second:
     ```python
     from pyspark import SparkContext
     from pyspark.streaming import StreamingContext

     sc = SparkContext("local[2]", "NetworkWordCount")
     ssc = StreamingContext(sc, 1)
     lines = ssc.socketTextStream("localhost", 9999)
     ```

2. **Processing Streaming Data:**
   - **DStreams:** Use Discretized Streams (DStreams) to represent the continuous data stream. DStreams allow you to perform high-level transformations on the data, such as mapping and reducing operations.
   - **Example:** You can use map operations to process each line of text, for instance, counting the number of words:
     ```python
     words = lines.flatMap(lambda line: line.split(" "))
     pairs = words.map(lambda word: (word, 1))
     ```

3. **Applying Machine Learning:**
   - **MLlib:** Integrate Spark's machine learning library (MLlib) to apply machine learning algorithms to the streaming data. MLlib provides various algorithms that can be applied to the data once it's processed into a suitable form.
   - **Example:** You might want to predict customer churn based on streaming data from a customer support chatbot. By processing the chat logs in real-time and applying machine learning models trained on historical data, you can predict the likelihood of churn for each customer, which can be used to trigger interventions or upsell opportunities.

4. **Performing Graph Analytics:**
   - **GraphX:** Use Spark's graph processing library, GraphX, to analyze the data graphically. This is particularly useful for applications that involve social networks, product recommendations, or any scenario where relationships between data points are important.
   - **Example:** If you're analyzing social media data streams, you could use GraphX to build and analyze social graphs, looking for influencers, communities, or other patterns that might be relevant for marketing or customer engagement.

5. **Deploying the Application:**
   - **Deployment Modes:** Choose an appropriate deployment mode for your application. You can deploy Spark on a standalone cluster, on YARN, or in Kubernetes. Each deployment mode has its own specific setup instructions, but all provide the necessary resources for running Spark applications.
   - **Example:** If you're deploying in a cloud environment, you might choose Kubernetes for its ability to manage resources efficiently and scale quickly. You can deploy your Spark application using the Spark Kubernetes Operator, which simplifies the deployment process.
   - **Configuration:** Customize your Spark configuration to optimize performance and resource usage. This includes setting the right memory allocations, tuning parameters, and using the latest Spark versions to take advantage of performance improvements.

By integrating these components and concepts, you can create a robust real-time data processing application that leverages Spark's streaming, machine learning, and graph processing capabilities. This approach not only processes data in real-time but also infuses intelligence into the processing through machine learning and provides a graphical overview of relationships and patterns via GraphX.