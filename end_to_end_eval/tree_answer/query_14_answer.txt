:

To optimize a Spark application involving both batch processing and real-time streaming data, and to ensure efficient resource allocation and performance across different deployment modes, while integrating machine learning models and SQL queries, one needs to leverage Spark's various components and follow a holistic approach as outlined below:

1. **Framework Selection:**
   - For batch processing, utilize Spark SQL, Datasets, and DataFrames, which provide optimized APIs for handling structured data and integrating with machine learning libraries like MLlib.
   - For real-time streaming, consider Structured Streaming, which is the newer and recommended API over the older DStreams for processing live data streams using DataFrames and Datasets.

2. **Deployment Mode:**
   - Choose the deployment mode based on the cluster setup and application requirements. Spark can be deployed in two primary modes:
     - **Client Mode:** Used when the driver program runs on a machine close to the worker nodes. Ideal for local execution or when the driver can communicate efficiently with the cluster.
     - **Cluster Mode:** Used when the driver program needs to run remotely from the worker nodes. This mode is beneficial for distributed teams or environments where the driver is not colocated with the cluster.

3. **Cluster Management:**
   - Use the appropriate cluster manager to handle resource allocation. Spark supports various cluster managers including Standalone, YARN, and Kubernetes. Each has its own configuration and capabilities, and can be selected based on the infrastructure and scalability needs.

4. **Resource Allocation:**
   - Ensure proper resource allocation by configuring Spark properties such as `spark.executor.cores`, `spark.executor.memory`, and `spark.executor.instances` to define the resources each executor can use.
   - Utilize dynamic resource allocation (introduced in Spark 3.0) to optimize resource usage by adjusting executor resources based on workload demands.

5. **Data Storage and Caching:**
   - Leverage Spark's in-memory caching for frequently accessed data to reduce computation time and network overhead. Use `dataFrame.cache()` or `spark.catalog.cacheTable("tableName")` to cache data in memory.
   - Integrate with cloud storage or distributed file systems for data persistence and scalability.

6. **Machine Learning Integration:**
   - Use MLlib to apply machine learning models on batch and streaming data. Models can be applied using transformations and actions on DataFrames or Datasets.
   - Consider the deployment of machine learning models on the cluster using Spark's ML Pipelines to automate the process and optimize resource usage.

7. **SQL Queries:**
   - Utilize Spark SQL to execute complex SQL queries on both batch and streaming data. This allows for easy integration with existing SQL-based systems and applications.
   - Configure Spark to run SQL queries using different execution plans (e.g., Shuffle or Merge) to optimize query performance.

8. **Monitoring and Tuning:**
   - Use Spark's built-in monitoring tools and web UI to track application performance and resource usage. This includes checking executor and task metrics to identify bottlenecks.
   - Follow the Spark tuning guide to optimize memory usage, execution plans, and parallelism levels.

By combining these strategies, one can create a Spark application that efficiently processes both batch and streaming data, leverages machine learning models for insights, and is capable of scaling across different deployment environments, ensuring optimal performance and resource utilization.