To ensure secure communication and custom plugin support in a Spark deployment, while handling large datasets, several configurations and considerations need to be put in place.

1. **Secure Communication:**
   - **Authentication:** Enable Spark's internal authentication by setting `spark.authenticate` to `true`. This ensures that all communications between Spark processes are authenticated.
   - **Secret Management:** For YARN and Kubernetes, Spark can manage the generation and distribution of unique authentication secrets for each application. This is crucial for maintaining security in multi-tenant environments.
   - **SSL/TLS:** Spark supports SSL/TLS for RPC channels, which can be enabled by setting `spark.io.ssl.enabled` to `true`. This encrypts the data in transit, protecting it from eavesdropping.
   - **Port Configurations:** Configure the `spark.connect.grpc.binding.port` and `spark.connect.grpc.interceptor.classes` properties to ensure that Spark Connect uses secure ports and supports custom interceptors for authentication or encryption.

2. **Custom Plugin Support:**
   - **Extensions:** Spark Connect supports custom extensions for Relations, Expressions, and Commands. These can be configured using properties like `spark.connect.extensions.relation.classes`, `spark.connect.extensions.expression.classes`, and `spark.connect.extensions.command.classes`.
   - **Dynamic Loading:** Plugins can be dynamically loaded at runtime using mechanisms provided by the programming language. For instance, in Python, plugins can be loaded using `spark.register_plugin()`.

3. **Large Dataset Considerations:**
   - **Caching:** Utilize Spark's caching mechanisms to store large datasets in memory efficiently. This is crucial for iterative algorithms or repeated data access.
   - **Partitioning:** Optimize partition sizes using configurations like `spark.sql.files.maxPartitionBytes` to balance memory usage and task parallelism.
   - **Broadcast Variables:** Broadcast large static datasets to minimize task size and improve job parallelism.

4. **Security Best Practices:**
   - **Authorization:** While Spark does not natively support authorization, it can be implemented using third-party tools or custom code. This is important for controlling access to data and resources.
   - **Secret Handling:** Ensure secure handling of authentication secrets, especially in environments like Kubernetes, where secrets may be exposed if not properly managed.
   - **Auditing and Logging:** Enable auditing and logging to track user activities and system events, facilitating security monitoring and incident response.

By combining these configurations and practices, a Spark deployment can achieve secure, customizable operations on large datasets, ensuring robust security and performance.