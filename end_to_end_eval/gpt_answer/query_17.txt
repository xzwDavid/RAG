
Spark's high availability (HA) setup ensures continuity and recovery in the event of a Master node failure by leveraging recovery mechanisms that preserve the cluster state and resume operations with minimal disruption. Below are the details for ZooKeeper-based and FILESYSTEM-based recovery modes, including their configurations and mechanisms:

1. High Availability with ZooKeeper
How It Ensures Continuity and Recovery

Leader Election: Multiple Master nodes are configured, and ZooKeeper is used to elect a leader among them. If the current leader dies, ZooKeeper automatically elects a new leader.
State Preservation: The cluster state, including applications and workers, is stored in ZooKeeper. The newly elected leader retrieves this state and resumes scheduling without disruption to running applications.
Notification Mechanism: The new leader contacts all previously registered applications and workers to inform them of the leadership change.
Key Features

Failover takes 1-2 minutes and only affects the scheduling of new applications.
Running applications are unaffected, as their state is preserved in ZooKeeper.
New Masters can be added or removed dynamically, ensuring flexibility.
Required Configurations

To enable ZooKeeper-based recovery, configure the following in the spark-env.sh file:

Enable Recovery Mode:
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER"
Specify ZooKeeper Connection Details:
ZooKeeper URL:
-Dspark.deploy.zookeeper.url=<zookeeper-hosts:port>
Example: zookeeper1:2181,zookeeper2:2181,zookeeper3:2181
ZooKeeper Directory:
-Dspark.deploy.zookeeper.dir=/spark-recovery
Master Configuration: Start multiple Masters on different nodes with the same ZooKeeper configuration. For example, provide multiple Masters to the SparkContext:
spark://host1:7077,host2:7077
2. High Availability with FILESYSTEM
How It Ensures Continuity and Recovery

The cluster state (applications and workers) is stored in a specified directory on the local or distributed filesystem (e.g., NFS).
If the Master node restarts, it reads the recovery state from the directory to resume operations.
Unlike ZooKeeper, this mode works for single-node recovery, and running applications remain unaffected.
Key Features

Suitable for non-production environments or lightweight setups.
Can be used with process managers like monit for auto-restart.
Supports file-based state persistence and optional compression.
Required Configurations

To enable FILESYSTEM-based recovery, configure the following in the spark-env.sh file:

Enable Recovery Mode:
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=FILESYSTEM"
Specify Recovery Directory:
-Dspark.deploy.recoveryDirectory=/path/to/recovery/directory
Optional Compression Codec: To compress recovery state files, set:
-Dspark.deploy.recoveryCompressionCodec=snappy
Timeout for Recovery: Specify the timeout for the recovery process:
-Dspark.deploy.recoveryTimeout=60s
Example Setup with NFS: If using an NFS directory for recovery:
Mount the NFS directory at /path/to/recovery/directory on all nodes.
Start the Master process on a new node in case of failure, pointing to the same recovery directory.
3. Comparison of ZooKeeper vs. FILESYSTEM Recovery Modes
Feature	ZooKeeper-Based Recovery	FILESYSTEM-Based Recovery
Use Case	Production clusters	Single-node recovery or non-production
State Storage	ZooKeeper	Local or distributed filesystem
Failover Time	1-2 minutes	Depends on restart process
New Masters	Can be added dynamically	Requires access to the recovery directory
Compression Support	Not applicable	Supported (e.g., snappy, zstd)
Dependencies	Requires a ZooKeeper cluster	No external dependencies
4. Common Gotchas and Recommendations
ZooKeeper-Based Recovery:

Misconfiguration of Masters: Ensure all Masters are configured with the same ZooKeeper URL and directory. If not, multiple Masters may think they are leaders, leading to a split-brain scenario.
ZooKeeper Availability: Deploy ZooKeeper with high availability (e.g., a quorum of 3 or 5 nodes).
Filesystem-Based Recovery:

State Cleanup: When changing the recovery mode or directory, manually clean up the old recovery state to avoid inconsistencies.
NFS for Multi-Node Recovery: If using NFS, ensure the filesystem is reliable and accessible from all potential Master nodes.
By properly configuring and choosing the right recovery mode for your use case, Spark ensures a robust high availability setup for uninterrupted operation.